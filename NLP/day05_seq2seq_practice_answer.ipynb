{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# seq2seq 모델"
      ],
      "metadata": {
        "id": "q0MlIIp8wfZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-L-7.max-800x600.jpg)"
      ],
      "metadata": {
        "id": "BYf-_1t1wiII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder 구현"
      ],
      "metadata": {
        "id": "SUO9UmWfwk2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-L-6.max-800x600.jpg)"
      ],
      "metadata": {
        "id": "lCPSBCPdwnNf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT83VDAopzxr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # 나는 밥을 먹었어\n",
        "        self.lstm = tf.keras.layers.LSTM(enc_units)\n",
        "    \n",
        "    def call(self, x):\n",
        "        print(\"입력 shape :\", x.shape) # sample input // 춤 추는 소시지 \n",
        "\n",
        "        x = self.embedding(x)\n",
        "        print(\"Embedding Layer를 거친 shape :\", x.shape)\n",
        "\n",
        "        output = self.lstm(x)\n",
        "        print(\"LSTM shape의 output shape :\", output.shape)\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "slWrhuY1wuw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 30000\n",
        "emb_size = 256\n",
        "lstm_size = 512\n",
        "batch_size = 1\n",
        "sample_seq_len = 3\n",
        "\n",
        "print(\"Vocab Size : {0}\".format(vocab_size))\n",
        "print(\"Embedding Size : {0}\".format(emb_size))\n",
        "print(\"LSTM Size : {0}\".format(lstm_size))\n",
        "print(\"Batch_size : {0}\".format(batch_size))\n",
        "print(\"Sample Sequence Length : {0}\".format(sample_seq_len))"
      ],
      "metadata": {
        "id": "2mgOnbTXxCMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bca9bfd-401e-43e8-c3ec-5503c87730e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size : 30000\n",
            "Embedding Size : 256\n",
            "LSTM Size : 512\n",
            "Batch_size : 1\n",
            "Sample Sequence Length : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_size, emb_size, lstm_size) #vocab_size, embedding_dim, enc_units):\n",
        "sample_input = tf.zeros((batch_size, sample_seq_len)) # 춤 추는 소시지\n",
        "\n",
        "sample_output = encoder(sample_input)"
      ],
      "metadata": {
        "id": "T0nOgRrexIzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d59eae2-b4ea-4a00-8e75-cc34dbfb11e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 shape : (1, 3)\n",
            "Embedding Layer를 거친 shape : (1, 3, 256)\n",
            "LSTM shape의 output shape : (1, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder 구현"
      ],
      "metadata": {
        "id": "YaCFAU30xPMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-L-7.max-800x600.jpg)"
      ],
      "metadata": {
        "id": "dFRBUyamxRYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(dec_units, return_sequences=True) ####\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "\n",
        "    def call(self, x, context_v):\n",
        "        print(\"입력 shape : \", x.shape)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        print(\"Embedding layer를 거친 shape\", x.shape)\n",
        "\n",
        "        context_v = tf.repeat(tf.expand_dims(context_v, axis=1), repeats=x.shape[1], axis=1)\n",
        "        x = tf.concat([x, context_v], axis = -1)\n",
        "        print('Context Vector가 더해진 shape : ', x.shape)\n",
        "\n",
        "        x = self.lstm(x)\n",
        "        print(\"LSTM layer의 output layer : \", x.shape)\n",
        "\n",
        "        output = self.fc(x)\n",
        "        print(\"Decoder의 최종 ouput layer : \", output.shape)\n",
        "\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "QNPaQgSVxNvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 30000\n",
        "emb_size = 256\n",
        "lstm_size = 512\n",
        "batch_size = 1\n",
        "sample_seq_len = 3\n",
        "\n",
        "print(\"Vocab Size : {0}\".format(vocab_size))\n",
        "print(\"Embedding Size : {0}\".format(emb_size))\n",
        "print(\"LSTM Size : {0}\".format(lstm_size))\n",
        "print(\"Batch_size : {0}\".format(batch_size))\n",
        "print(\"Sample Sequence Length : {0}\".format(sample_seq_len))"
      ],
      "metadata": {
        "id": "zmLtmubHxzyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363a0d7f-985b-40ef-faee-f80d0aa2eb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size : 30000\n",
            "Embedding Size : 256\n",
            "LSTM Size : 512\n",
            "Batch_size : 1\n",
            "Sample Sequence Length : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_size, emb_size, lstm_size)\n",
        "sample_input = tf.zeros((batch_size, sample_seq_len)) # 춤 추는 소시지\n",
        "\n",
        "sample_output = decoder(sample_input, sample_output)"
      ],
      "metadata": {
        "id": "ZwalJJzKx0W-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd35f4f4-930e-4a2d-e2f6-97161a248233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 shape :  (1, 3)\n",
            "Embedding layer를 거친 shape (1, 3, 256)\n",
            "Context Vector가 더해진 shape :  (1, 3, 768)\n",
            "LSTM layer의 output layer :  (1, 3, 512)\n",
            "Decoder의 최종 ouput layer :  (1, 3, 30000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.array([[1,2], [3,4]])\n",
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6X9G91AA1pW",
        "outputId": "643bba74-5e7a-40f3-a69e-4431e6cc9a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.repeat([[1,2], [3,4]], repeats=[2,3], axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQct9w7yAdIn",
        "outputId": "7c246a18-4776-4d45-d258-78e28903847b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 2), dtype=int32, numpy=\n",
              "array([[1, 2],\n",
              "       [1, 2],\n",
              "       [3, 4],\n",
              "       [3, 4],\n",
              "       [3, 4]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.repeat([[1,2], [3,4]], repeats=[2,3], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F1ELNWOAyXu",
        "outputId": "4b914e6a-2243-44be-831e-add7efba732e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 5), dtype=int32, numpy=\n",
              "array([[1, 1, 2, 2, 2],\n",
              "       [3, 3, 4, 4, 4]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.repeat(3, repeats=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QehAk4jBIKv",
        "outputId": "549941a0-b3ff-48f1-9c20-abd8fe648ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 3, 3, 3], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention"
      ],
      "metadata": {
        "id": "I2rWe11Hyake"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bahdanau Attention\n",
        "$$ Score_{alignment} = W * tanh(W_{decoder} * H_{decoder} + W_{encoder} * H_{encoder}) $$"
      ],
      "metadata": {
        "id": "HVVLgiLHycqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "W2CJ2qliyaxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.w_decoder = tf.keras.layers.Dense(units)\n",
        "        self.w_encoder = tf.keras.layers.Dense(units)\n",
        "        self.w_combine = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, h_encoder, h_decoder):\n",
        "        print(\"[H_encoder] shape : \", h_encoder.shape)\n",
        "\n",
        "        h_encoder = self.w_encoder(h_encoder)\n",
        "        print(\"[w_encoder x h_encoder] shape : \", h_encoder.shape)\n",
        "\n",
        "        print(\"\\n[H_decoder shape :\", h_decoder.shape)\n",
        "        h_decoder = tf.expand_dims(h_decoder, 1)\n",
        "        h_decoder = self.w_decoder(h_decoder)\n",
        "\n",
        "        print(\"[w_encoder x h_decoder] shape :\", h_decoder.shape)\n",
        "\n",
        "        score = self.w_combine(tf.nn.tanh(h_encoder+h_decoder))\n",
        "        print(\"[score alinment] shape :\", score.shape)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        print(\"\\n최종 weight : \\n\", attention_weights.numpy())\n",
        "\n",
        "        context_vector = attention_weights * h_decoder\n",
        "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "9csARr_vyftu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_size = 100\n",
        "print(\"hidden state {0}차원으로 Mapping \\n\".format(w_size))\n",
        "\n",
        "attention = BahdanauAttention(w_size)\n",
        "\n",
        "enc_state = tf.random.uniform((1, 10, 512))\n",
        "dec_state = tf.random.uniform((1, 512))\n",
        "\n",
        "_ = attention(enc_state, dec_state)"
      ],
      "metadata": {
        "id": "TdSyZrAnzUQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb232641-aee0-48a0-f83b-00ed0bc3744b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden state 100차원으로 Mapping \n",
            "\n",
            "[H_encoder] shape :  (1, 10, 512)\n",
            "[w_encoder x h_encoder] shape :  (1, 10, 100)\n",
            "\n",
            "[H_decoder shape : (1, 512)\n",
            "[w_encoder x h_decoder] shape : (1, 1, 100)\n",
            "[score alinment] shape : (1, 10, 1)\n",
            "\n",
            "최종 weight : \n",
            " [[[0.09503427]\n",
            "  [0.16995946]\n",
            "  [0.08780645]\n",
            "  [0.11135209]\n",
            "  [0.07796121]\n",
            "  [0.12292695]\n",
            "  [0.09000642]\n",
            "  [0.13369563]\n",
            "  [0.06222299]\n",
            "  [0.04903455]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://aiffelstaticprd.blob.core.windows.net/media/original_images/GN-4-L-9.jpg)"
      ],
      "metadata": {
        "id": "E-Fw7WBDzX18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loung Attention"
      ],
      "metadata": {
        "id": "eSfQTfeGzbLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ Score(H_{target},H_{source}) = H_{target}^T * W_{combine} * H_{source}$$"
      ],
      "metadata": {
        "id": "gu8BHyqmzc2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.w_combine = tf.keras.layers.Dense(units)\n",
        "\n",
        "    def call(self, h_encoder, h_decoder):\n",
        "        print(\"[h_encoder] shape : \", h_encoder.shape)\n",
        "\n",
        "        wh = self.w_combine(h_encoder)\n",
        "        print(\"[W_encoder x h_encoder] shape :\", wh.shape)\n",
        "\n",
        "        h_decoder = tf.expand_dims(h_decoder, 1)\n",
        "        alignment = tf.matmul(wh, tf.transpose(h_decoder, [0, 2, 1]))\n",
        "        print(\"[Score alignment] shape : \", alignment.shape)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(alignment, axis=1)\n",
        "        print(\"\\n 최종 weight : \\n\", attention_weights.numpy())\n",
        "\n",
        "        attention_weights = tf.squeeze(attention_weights, axis= -1)\n",
        "        context_vector = tf.matmul(attention_weights, h_encoder)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "UO1Xn8V5zbXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 512\n",
        "attention = LuongAttention(emb_size)\n",
        "\n",
        "enc_state = tf.random.uniform((1, 10, emb_size))\n",
        "dec_state = tf.random.uniform((1, emb_size))\n",
        "\n",
        "_ = attention(enc_state, dec_state)"
      ],
      "metadata": {
        "id": "qd-KxPyKzo5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22258f0-d86f-4c8b-cebb-0258ec737204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[h_encoder] shape :  (1, 10, 256)\n",
            "[W_encoder x h_encoder] shape : (1, 10, 256)\n",
            "[Score alignment] shape :  (1, 10, 1)\n",
            "\n",
            " 최종 weight : \n",
            " [[[1.9313217e-04]\n",
            "  [1.2533090e-03]\n",
            "  [5.0220143e-02]\n",
            "  [2.2237128e-01]\n",
            "  [4.4970747e-02]\n",
            "  [4.3029017e-03]\n",
            "  [3.2295686e-01]\n",
            "  [6.2566027e-03]\n",
            "  [2.2300805e-03]\n",
            "  [3.4524491e-01]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFukkslPQdoQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}